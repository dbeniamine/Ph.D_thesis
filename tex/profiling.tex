%!TEX encoding=UTF-8 Unicode
\chapter{Performances analysis}

\section{Modern computing systems}

\subsection{Architectural considerations}

\DB{Caches, prefetechrs, NUMA few words about GPGPU, MIC \ldots}

\subsection{Usual performances issues}

\DB{All to all sharing, bad Mappings, false sharing \ldots}


\section{Profiling tools}

\DB{Main Biblio}

\subsection{Performance counters}

\DB{perfctr}

\subsection{Generic tools}

\DB{HPC toolkit etc}

\subsection{Discussion}

\section{Experimental Methodology}

While in other domains such as biology experiments takes a large amount of
time and money, they are almost inexpensive and can be designed very quickly
in computer science. As a results, we computer scientists, are not used to
take the time to design correct experiments trying to minimize bias and using
the right tools. Furthermore as hardware and software evolves very quickly we
usually does not bother to re run and verify the results presented previous
studies while it is the best way to spot bias or confirm the validity of an
experimental claim.

In this section we first introduce why reproducibility matters and how people
have try to reach it in \gls{hpc}, then we present the methodology we have
developed during this thesis to make our experiment as reproducible as
possible.


\subsection{Reproducible research}

Measurement bias is a widely known phenomena in scientific communities and is
analyzed in most fields. Still in \gls{hpc} the only thing we usually do to avoid it
is to run a large number of experiments hopping that if we have enough
observations on several configurations the measurement bias will be
negligible. Mytkowicz et al.~\cite{Mytkowicz09Producing} highlighted several
way to introduce significant measurement bias in computer science experiments
without noticing it, showing that measurement bias is commonplace and
unpredictable in our field.

As measurement bias is unpredictable even when we follow the best practices
the easiest way to deal with it is to reproduce the study published by other
team and confirm or invalidate their results. While this is done in every
scientific fields, it is not common to publish about experiment reproduction
in \gls{hpc}.

A previous study~\cite{Collberg15Repeatability} tried to evaluate how
reproducible the experiment presented in computer science article are. To do
so, they focused on the capacity to build the experimental code and evaluated
$601$ articles published in “top ACM conferences and journal”. From these
$601$ articles they were only able to build the environment of $217$ articles.
Moreover it took more than half an hour to build $64$ of these papers and $23$
other required the intervention of the authors.

At this point we need to define precisely reproducibility, for the remaining
of this thesis, except if specified otherwise, we will use the definition
proposed by Dror G. Feitelson~\cite{Feitelson15From}:

\begin{quote}
    Repeatability concerns the exact repetition of an experiment, using the
    same experimental apparatus, and under the same conditions.

    Reproducibility is the reproduction of the gist of an experiment:
    implementing the same general idea, in a similar setting, with newly
    created appropriate experimental apparatus.
\end{quote}

It is nearly impossible to repeat an experiment in the exact same conditions
as many exterior factors, such as the machine room temperature or the network
usage done by other users on the same cluster, might impact the measured
performances.\DBm{Citations ? Demander a Arnaud ?}
Still it is possible to repeat an experiment in similar conditions: someone
who runs the same experiments two different days on the same machines can
expect to obtain similar results. Several tools can helps use making
experiments more repeatable for instance by running our experiment on a shared
platform such as grid5000~\cite{Cappello05Grid5000} we can hope that other
people might access to the same set of machines. Moreover deploying a custom
environment on these machines allow to control and distribute the set of
installed libraries. Kameleon~\cite{Ruiz15Reconstructable} go even
further as it allows to describe an environment as a recipe thus makes it easy
to check the version of a library or replace it.

To reproduce an experiment it is important to understand how it has been
designed and how it evolves from the first version to the results presented in
the paper. Stanisic et al.~\cite[Chapter~4, p31-44]{Stanisic15Reproducible}
described an experimental workflow based on \emph{git} and \emph{emacs orgmode}
to keep track of these evolutions and make easy for someone to understand it.
One of the main drawback of this workflow is that it is not suitable for
experiment which generate huge ($\ge$\SI{500}{Mib}) trace files as git is not
designed to handle such files.

Several tools were designed to conduct experiments in computer science still
they are not designed for \gls{hpc} and using them in our context would
require some adjustments. A more complete survey can be found
in~\cite[Chapter~3, p17-19]{Stanisic15Reproducible}

\subsection{Experimental workflow}

We design experiments to answer simple questions such as ``Is my tool more
efficient than the existing ones'', obviously this formulation is too vague to
be answered scientifically. We first need to find a set of relevant state of
the art tools and then decide and on which configurations we are going to do
the comparison. This means choosing a set of benchmarks representing real life
applications, one or several credible experimental machine(s). Finally we must
determine a set of metrics that we can measured accurately to evaluate the
efficiency of the tools.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{tikz/exp-tikz}
    \caption{Experimental pipeline}
    \label{fig:exp-pipeline}
\end{figure}

All these choices form a high level \emph{experimental plan}: it contains all
the information required to reproduce the experiment but not to repeat it.
Indeed it miss the code that actually execute the experimental plan.
We can see an experiment as a pipeline that flow from this plan to the final
human readable results as described in \fig{exp-pipeline}. This pipeline is
composed of three steps: the experimental plan that produce a (usually) huge
set of data and meta data, the first one are meant to be used during the
analysis while the second are only generate to find issue in the experiment or
additional information to repeat it. Then comes the raw analysis that extract
pertinent information and compute metrics from the raw data. The results of
this step are usually easy to parse for a computer but not human friendly.
Thus the last step is there to present these data in a readable way.

Yet it is easy to present data in a tendentious way that reflects more our
expectations than the reality without even noticing it, we call this the
\emph{experiment designer bias}\DBm{Ref ?}. To limit this bias,
it is very important to design carefully the whole analysis before running the
actual experiment and test the visualisation on a filler set of data.
Furthermore, adding comments to the visualisations about what they mean and
what we expect to see can help avoiding this bias. Technologies such as
\texttt{R-markdown},\texttt{Jupyter Notebook} (anciently known as IPython
Notebook) or \texttt{Emacs Orgmode} are great help to produce reproducible
experiments.

\subsubsection{Methodology}

\paragraph{Preparing an experiment:}

In this thesis, we ran two types of experiments: the fist one aims at
analysing the behavior of one application (mostly \Gls{sofa}) while the second
one aims at comparing trace collection tools. For both analysis we need to
find a set of \emph{benchmarks} conduct our experiments, but the signification
of this term is slightly different for the two cases. In the first case, a
benchmark is an input for the studied application and the set of benchmarks
used should be defined with the developers of the application to cover the
code that they want to improve. While in the second case it means a set of
applications, for this need, we use the \gls{npb}~\cite{Jin1999} which allow
to cover a large set of applications from simple computational kernels to real
life applications. Yet to compare several analysis tools, a set of benchmarks
is not a sufficient input as these tools usually provides a large set of
parameters allowing to modify their behavior, thus we must evaluate each tool
under at least two set of parameters: the default and a tuned version with a
set of parameters optimized for the results we are measuring.

Once the benchmarks are chosen, we need to decide of one machine or a set of
machines on which we will run the experiment. This step is crucial as
computers architectures are getting more and more complex \DB{Ref partie 2.1
?} and applications are (usually) designed for one type of machines.
Furthermore some tools (such as \gls{Pin}\DBm{Citation ?}, \gls{PEBS}, \gls{IBS}) are vendor
specific or optimized for some architecture. Thus we must find machines that
allow to do our study in a relevant situation and fair to the tools and
libraries used.

Finally we need to find some relevant metric(s) to answer the question(s) that we
are trying to answer with our experiment. These metrics should remain as
simple as they will be interpreted by humans. Yet they must also cover every
aspects that we are studying. A complete set of simple measurable metrics is
often easier to understand that one complex metric that provide an overall
score in an unknown unit. Still simple ratios and percentages can make an
analysis more reproducible for instance an execution time is significant only
for a very specific configuration on one machine, while speedups and overheads
can be compared more easily.


\paragraph{Writing the experiment scripts:}

It is crucial that any step of the experiment from the deployment of the
environment to the final analysis is properly scripted in a language that can
be understood by an other developer. Any manual step can make the experiment
hard to reproduce and an obscure code might make it hard to change some
parameters.

Furthermore the traces generated by an experiment must be self explanatory or
at some point they will only be a (large) set of meaningless bytes on a hard
drive. To do so it is important to keep together the data and both the scripts
that generated it and the one that allows to interpret its. To do so, all our
experimental scripts starts the same way: they create a new directory that
will hold the traces, copy themselves and all the scripts which are on their
directory inside it. Then they duplicates their output to a file in this
experimental directory and Log every sensible meta data such as the
command that started the experiment, the machine name, its topology, \gls{OS}
the environment variables, the status of every dependencies (git version, diff
etc.). Such informations can be a great help to someone trying to reproduce
the experiment or to verify that the experimental condition are not biased.

The actual execution of the experiment consist of a set of runs, to limit the
impact of external noise, it is a good practise to randomize these runs.
Therefor our experimental scripts generate the list of runs that should be
executed shuffle it and then execute it run in a randomized order. A run
consist of three step: a pre command that can set specific values for the run,
the actual command that execute the benchmark and the post command that can
save data, might compute metrics and must restore the normal state. Note that
the first and last steps are not mandatory. Each of the commands executed by
these 3 steps are printed before execution thus logged.

At the end of the day our experiment have produced a huge set of data that we
can divide on two categories: the meta data that are useful for debugging and
reproducibility and the raw results that will be filtered to then be analyzed.
The parsing step is designed only to extract data from the traces so the
analysis scripts can work on clean results, we avoid to do any statistic
during this step.

Finally for the analysis \texttt{R} provides a large set of reliable statistic
analysis libraries and technologies such as \texttt{R-markdown} or
\texttt{Orgmode} allows to mix analysis code and results with comments. These
technologies allow to produce a standalone result (as a pdf or as a webpage)
that contains the original questions, our assumptions, the results and plots,
and our observation on these results.

\paragraph{Distributing the experiment:}

Trying to compile a badly documented code can be painful and time consuming,
the same problems occurs when it comes to reproduce experiment. Thus the
distribution of the experiment traces and script must be considered carefully.
To reproduce an experiment with need the scripts that run it, a description of
the machines and/or the virtual environment that is deployed and a link to the
right version of each tool, applications and benchmarks used during the
experiment. Several services such as Github, Bitbucked, and Gitlab \DBm{Urls ?}
host freely code repositories, furthermore \glspl{VCS} such as Git and
Mercurial \DBm{Citations ?} allow to track easily dependencies with a system
of submodules, therefore they are well suited for experiment distribution.
Finally an effort must be done on the documentation to help the user
understand why the experiment is designed, how it works and where (on which
machines) we should run it.

Distributing the experiment results can be more tricky as these files can be
quite heavy and code repositories are usually not designed to handle large
files. While the experimental repository allows to reproduce the whole
experiment, the trace distribution is useful to reproduce only some specific
steps of the analysis, thus we distribute to set of files corresponding to the
two analysis steps depicted in \fig{exp-pipeline}. The first one contains all
the files generated by the experiment and the scripts to run that analysis.
This distribution can be quite heavy (around \SI{100}{Gib} for some of our
experiments) but allows to redo the whole analysis, compute new metrics or
look for additional informations in the meta data. At the opposite, the second
one only contains the filtered results and the scripts that generate the
plots, it allows to redo the statistical analysis exploring the data with a
different approach. Furthermore this trace usually contains more plots and
data than a published article that have to fit a certain amount of pages. Such
traces can be distributed using specialized large file hosting services such
as Renater File Sender, or Zenodo.\DBm{Url, Citation ?}


\section{Sofa: case study}

\subsection{Sofa}

\DB{Sofa particularities, different improvements approach (Everton / Julio)}

\subsection{Experimental analysis}

\DB{Application of the methodology presented above}

\subsection{Results and Discussions}

\DB{Need memory oriented tools, improve the methodology}


\subsection{Proposition}

\DB{Main goals of a memory analysis tool}
