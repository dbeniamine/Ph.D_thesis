%!TEX encoding=UTF-8 Unicode
\chapter{Performances analysis}

\section{Modern computing systems}

\subsection{Architectural considerations}

\DB{Caches, prefetechrs, NUMA few words about GPGPU, MIC \ldots}

\subsection{Usual performances issues}

\DB{All to all sharing, bad Mappings, false sharing \ldots}


\section{Profiling tools}

\DB{Main Biblio}

\subsection{Performance counters}

\DB{perfctr}

\subsection{Generic tools}

\DB{HPC toolkit etc}

\subsection{Discussion}

\section{Experimental Methodology}

While in other domains such as biology experiments takes a large amount of
time and money, they are almost inexpensive and can be designed very quickly
in computer science. As a results, we computer scientists, are not used to
take the time to design correct experiments trying to minimize bias and using
the right tools. Furthermore as hardware and software evolves very quickly we
usually does not bother to re run and verify the results presented previous
studies while it is the best way to spot bias or confirm the validity of an
experimental claim.

In this section we first introduce why reproducibility matters and how people
have try to reach it in \gls{hpc}, then we present the methodology we have
developed during this thesis to make our experiment as reproducible as
possible.


\subsection{Reproducible research}

Measurement bias is a widely known phenomena in scientific communities and is
analyzed in most fields. Still in \gls{hpc} the only thing we usually do to avoid it
is to run a large number of experiments hopping that if we have enough
observations on several configurations the measurement bias will be
negligible. Mytkowicz et al.~\cite{Mytkowicz09Producing} highlighted several
way to introduce significant measurement bias in computer science experiments
without noticing it, showing that measurement bias is commonplace and
unpredictable in our field.

As measurement bias is unpredictable even when we follow the best practices
the easiest way to deal with it is to reproduce the study published by other
team and confirm or invalidate their results. While this is done in every
scientific fields, it is not common to publish about experiment reproduction
in \gls{hpc}.

A previous study~\cite{Collberg15Repeatability} tried to evaluate how
reproducible the experiment presented in computer science article are. To do
so, they focused on the capacity to build the experimental code and evaluated
$601$ articles published in “top ACM conferences and journal”. From these
$601$ articles they were only able to build the environment of $217$ articles.
Moreover it took more than half an hour to build $64$ of these papers and $23$
other required the intervention of the authors.

At this point we need to define precisely reproducibility, for the remaining
of this thesis, except if specified otherwise, we will use the definition
proposed by Dror G. Feitelson~\cite{Feitelson15From}:

\begin{quotation}
    Repeatability concerns the exact repetition of an experiment, using the
    same experimental apparatus, and under the same conditions.

    Reproducibility is the reproduction of the gist of an experiment:
    implementing the same general idea, in a similar setting, with newly
    created appropriate experimental apparatus.
\end{quotation}


It is nearly impossible to repeat an experiment in the exact same conditions
as many exterior factors, such as the machine room temperature or the network
usage done by other users on the same cluster, might impact the measured
performances.\todo[author=david]{Citations ? Demander a Arnaud ?}.
Still it is possible to repeat an experiment in similar conditions: someone
who runs the same experiments two different days on the same machines can
expect to obtain similar results. Several tools can helps use making
experiments more repeatable for instance by running our experiment on a shared
platform such as grid5000~\cite{Cappello05Grid5000} we can hope that other
people might access to the same set of machines. Moreover deploying a custom
environment on these machines allow to control and distribute the set of
installed libraries. Kameleon~\cite{Ruiz15Reconstructable} go even
further as it allows to describe an environment as a recipe thus makes it easy
to check the version of a library or replace it.

To reproduce an experiment it is important to understand how it has been
designed and how it evolves from the first version to the results presented in
the paper. Stanisic et al.~\cite[Chapter~4, p31-44]{Stanisic15Reproducible}
described an experimental workflow based on \emph{git} and \emph{emacs orgmode}
to keep track of these evolutions and make easy for someone to understand it.
One of the main drawback of this workflow is that it is not suitable for
experiment which generate huge ($\ge$\SI{500}{Mib}) trace files as git is not
designed to handle such files.

Several tools were designed to conduct experiments in computer science still
they are not designed for \gls{hpc} and using them in our context would
require some adjustments. A more complete survey can be found
in~\cite[Chapter~3, p17-19]{Stanisic15Reproducible}

\subsection{Experimental workflow}

To design repeatable and or reproducible experiment it is important to
consider the whole workflow, we describe it as a three step pipeline depicted
in \fig{exp-pipe}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{tikz/exp-tikz}
    \caption{Experimental pipeline}
    \label{fig:exp-pipe}
\end{figure}

\begin{itemize}
    \item Focus on experiment and analysis reproducibility (not
        feedback/experiment loop)
    \item See experiment as a workflow described in \fig{exp}
    \item Goal: Make all the workflow repetable or at least reproducible
        \begin{enumerate}
            \item Setup
                \begin{itemize}
                    \item Make a plan:
                        \begin{itemize}
                            \item Case study (benchmarks: application representing real life
                                problems).
                            \item Configurations (different version of the benchmarks code,
                                runtimes with different set of parameters).
                            \item Experimental machines / environment (recent enough,
                                representative AMD / Intel ?, several OS versions ?)
                        \end{itemize}
                    \item Prepare analysis:
                        \begin{itemize}
                            \item What do we want to know ? Which metrics can answer our
                                questions ?
                            \item Avoid execution time, prefer overhead, speedup, slowdown
                        \end{itemize}
                    \item Store everything in a git repository
                    \item Use git submodule for dependencies
                    \item Choice of metrics
                    \item Correct documentation
                    \item An init script can apply patches if required
                    \item Launch scripts responsible for the experiment
                        deployment should be included
                \end{itemize}
            \item Raw analysis
                \begin{itemize}
                    \item Simpler step but crucial
                    \item Extract important information from raw traces
                    \item Do not compute thing here
                    \item Heavy files cannot be in a git repository
                    \item Use online storage (for instance zenodo)
                    \item A Readme should explain file hierarchy
                    \item Include filtering and analysis scripts
                \end{itemize}
            \item Statistic analysis
                \begin{itemize}
                    \item Mix text with code (R-markdown, orgmode \ldots)
                    \item Prepare it before run
                    \item Repeat original question and expected results
                    \item Use online storage (for instance zenodo)
                    \item A Readme should explain file hierarchy
                    \item Include analysis scripts
                \end{itemize}
            \item And now ?
                \begin{itemize}
                    \item Raise more question => new experiments
                    \item Enough answers => write paper
                \end{itemize}
        \end{enumerate}
\end{itemize}

Actual experiment (on the experimental machine)
\begin{enumerate}
    \item Log every thing
        \begin{itemize}
            \item Machine (name, topology, OS \ldots, OS
                config)
            \item Dependencies status (git version, diff etc.)
                script diff
            \item Environment variables, script arguments etc.
        \end{itemize}
    \item Preparation
        \begin{itemize}
            \item Clean then compile every thing
            \item Set environment variables
            \item Prepare ordrer of runs (if randomization)
        \end{itemize}
    \item Execute Runs:
        \begin{enumerate}
            \item PreCommand: set specific values for the run
            \item Run: do the experiment
            \item PostCommand: Save data, compute metrics,
                restorenormal state
        \end{enumerate}
        precommand and postcommand must cancel each other
\end{enumerate}

\DB{My description of experimental workflow / how to make it more
reproducible}

\section{Sofa: case study}

\subsection{Sofa}

\DB{Sofa particularities, different improvements approach (Everton / Julio)}

\subsection{Experimental analysis}

\DB{Application of the methodology presented above}

\subsection{Results and Discussions}

\DB{Need memory oriented tools, improve the methodology}


\subsection{Proposition}

\DB{Main goals of a memory analysis tool}
