%!TEX encoding=UTF-8 Unicode
\chapter{Performances analysis}
\label{chap:perf}

\begin{itemize}
    \item Aim: improving application without a priori knowledge
    \item Requires to have knowledge on the  machine architecture
    \item Profiling is an experiment blabla
\end{itemize}

This chapter is organised as follow: first we present some specificities of
modern computing system that must be considered while trying to improve an
application's performances in \sect{mod-syst}, then in \sect{prof-tools} we
present some of the existing profiling tools, discuss their strength and
limits. After that we detail our experimental methodology and discuss
reproducibility matters in \sect{expe-methodo}. Finally we present a study
case where we evaluated the performances of a physical simulation application
in \sect{sofa}.

\section{Modern computing systems}
\label{sec:mod-syst}

\subsection{Architectural considerations}

\DB{Caches, prefetechrs, NUMA few words about GPGPU, MIC \ldots}

\subsection{Usual performances issues}

\DB{All to all sharing, bad Mappings, false sharing \ldots}


\section{Profiling tools}
\label{sec:prof-tools}


\begin{itemize}
    \item Two steps
        \begin{itemize}
            \item collection
            \item Presentation
        \end{itemize}
    \item Several categories:
        \begin{itemize}
            \item Low level (perf counters)
            \item High level (Generic tools)
            \item Specific tools (High or low level)
        \end{itemize}
    \item How to get a global overview of an application's performances ?
\end{itemize}

Most profiling tools are based on performance counters, these counters are
implemented using dedicated \gls{CPU} registers. This hardware based design
allow to retrieve information on the execution with low intrusively and cost.
Yet the available counters depends on the \gls{CPU} vendor and model,
furthermore it is only possible to use a restricted number of counters at
the same time.
\DB{Est-ce que j'ajoute que ya des compteurs cachés qu'on ne peux utiliser
qu'avec des outils propriétaires ?}

Performance counters can directly be accessed using the \gls{Perfctr} driver
which is part of the \gls{Linux} kernel since version 2.6.x\DBm{Vérifier
version, citation perfctr ?}, 
%% Vendor specific
%% Hard to interpret

yet higher level library such as
\gls{PAPI}~\cite{Browne00Portable,Malony11Parallel,Weaver13PAPI} and
\gls{Likwid}~\cite{Treibig10LIKWID} were designed to compute metrics based on
these counters in a more portable way. Both of these tools allow to use
predefined performance groups, start and stop the counters during the
execution to focus on critical parts of the execution. Although this last
ability can be very useful to focus on the most important part of the code, it
might lead to missing out inefficient code.

%% Global trace plus context
%%  Retrieve context infos:
%%      Hooks librarys / syscall (slow)
%%      Instrumentation (slower)

%% Oprofile / Persuite
%% TAU

%% Lots of Value, complexe data => Visualization

Performances counter were originally designed by vendors to debug their
processors prototypes, as a result, interpreting performance counters requires
a deep knowledge of \glspl{CPU}, that most computer scientist do not have. The
capability to group counters and compute metrics provided by \gls{PAPI} and
\gls{Likwid} mitigate this limitations.



Finally \gls{OProfile}~\cite{Oprofile}
allows the user to retrieve a profile of an application detailing the values
of the performance counters on different parts of the program.

An other approach to overpass the limitations of performances counters consist
on combining them with other informations obtain by hooking \glspl{syscall} or
calls to the standard C library and other usual libraries such as \gls{MPI} or
\gls{OpenMP}.

Yet these methods are more intrusive thus slower and might
impact the behavior of the studied application.


%% Other collection Methods:

Instrumentation (Pin, Paradyn)

PerfSuite => PAPI + Glibc wrapper


TAU: advanced trace measurement, no visu (Vampir)


All the above tools address the challenge of data collection but not the
presentation of results, thus the user have to go through the trace by itself
to extract the pertinent information.

%%%%%%%%% Higher level tools

Visu only:
    + Vampir
    + FrameSoc: Trace storage + visualization, nothing for collection

All In one tools: Perfctr + Simu / instru + Visu

Vendors:
    + AMD CodeAnalyst / CodeXL
    + Intel VTune (proprietaty)

Research:
    + Complete framework / plugable
        + HTPC Toolkit
        + MAQAO
        + Paraver
            + DINEMAS
            + Paramedir
    + MERLIN: Automatic performance advisor
    + Scalasca: For massively parrallel systems

Several generic tools have been designed to analyze and improve parallel
applications performances, such as \gls{PerfSuite}~\cite{Kufrin05Perfsuite}
\gls{Intel}'s \gls{VTune}~\cite{Reinders05VTune},
\gls{PCM}~\cite{Wilhalm16Intel}, the
\gls{HPCToolkit}~\cite{Adhianto10HPCTOOLKIT}, and \gls{AMD}'s
\gls{CodeAnalyst}~\cite{Drongowski08introduction}. All of these tools rely on
performance counters and execution traces to show when and where CPUs are
idle, and, thus, highlight potential places for improvements.
\gls{CodeAnalyst}'s successor \gls{CodeXL}~\cite{AMDCodeXL} is also able to
trace and display \gls{GPU} related performance informations.

\DB{From biblio:}
The following tools provides at least the same informations as performance
counters but display it in a nicer way.
\begin{itemize}
    %\item Vtune \cite{Reinders05VTune} is the Intel tool for analysing
    %    performances. It allows the user to design a lot of different
    %    analysis and give the results in different views. VTune always tries
    %    to show the origin of the problem and to give hints. It also allow the
    %    user to map the counters to binary or high level code. However it is a
    %    very complex program, most informations are displayed as huge tables.
    %    And some features as the bandwith analysis seems to be broken.
    %\item HPCToolkit \cite{Adhianto10HPCTOOLKIT} is one of the most complete
    %    toolkit. It respect the following principles:
    %    \begin{itemize}
    %        \item Avoid code instrumentation
    %        \item Language independent
    %        \item Try to correlate data (for instance cache miss rate \&
    %            cache latency)
    %        \item Combine many techniques
    %            \begin{itemize}
    %                \item Binary analysis
    %                \item Statistical sampling
    %                \item Event triggers
    %            \end{itemize}
    %    \end{itemize}
    %    However this framework is very complex to use, it does not allow the
    %    user to easily build new tools, and it focuses on processors events.
    \item MAQAO \cite{Djoudi05MAQAO}
        \begin{itemize}
            \item Consider themself as HPCToolkit concurrent
            \item Works on compilers binary
            \item Very interesting state of the art
            \item Pattern recognition inside assembly code
        \end{itemize}
        However only works on the binary \dots
    \item Paradyn \cite{Miller95Paradyn}
        \begin{itemize}
            \item Dynamic instrumentation library
            \item Search model and performance consultant
                \begin{itemize}
                    \item Three axes:
                        \begin{itemize}
                            \item Why: what kind of pb
                            \item Where: code / data centric
                            \item When: function call graph
                        \end{itemize}
                    \item Predefine statical hypotheses to explain perf issue
                \end{itemize}
            \item Old paper 95 nothing about perf bug research on website
        \end{itemize}
    \item Paraver + DINEMAS \cite{Pillet95PARAVER} visualization and
        simulation tools. Standard interface for visualization allow to plug
        custom tools inside Paraver. Provide the capability to define custom
        metrics.
    \item Paramedir \cite{Jost04Paramedir} CLI tool which allow to display
        ASCII representation of Paraver trace.
    \item Vampir  \cite{Brunst01Performance} %also described in \cite{Sunderl07Profiling}.
        Commercial trace visualization tool.
        \begin{itemize}
            \item Designed for distributed systems: hierarchichal view (Gantt)
            \item Call stack infos
        \end{itemize}
    \item ATUM \cite{Agarwal86ATUM}
        \begin{itemize}
            \item HW trace using microcode
            \item Arch dependent
            \item Old paper, obsolete methode: perf counters
        \end{itemize}
    \item URSA MINOR \cite{Park00Supporting}
        \begin{itemize}
            \item Trace analyze + guide using MERLIN\cite{Kim01Performance}
            \item Interactive visualisation (big table and ugly pies)
            \item No website ?
        \end{itemize}
    \item MERLIN \cite{Kim01Performance}
        \begin{itemize}
            \item Automatic performance advisor
            \item Same kind of guide as paradyn \cite{Miller95Paradyn} /
                Vtune \cite{Reinders05VTune}
        \end{itemize}
    \item TAU \cite{Shende06Tau} is one of the most known measurement tools
        \begin{itemize}
            \item Provides lot of API for instrumentation
            \item Visualization with
                VAMPIR~\cite{Nagel96VAMPIR} or paraver
        \end{itemize}
    \item Scalasca \cite{Geimer10Scalasca} is very similar to Tau, but
        designed to improve scalability, indeed it aims at massively parallel
        systems.
\end{itemize}

All these tools focuses on the CPU while most performance problems comes from
communication (network and/or memory).
Malony et al. \cite{Malony11Parallel} adds support for CUDA in classical
performance system (TAU, PAPI, VAMPIR)

\begin{itemize}
    \item Basically counters + Visu
    \item Often complex to use
    \item Vtunes proprietary, horrible license
\end{itemize}

\subsection{Discussion}

Profiling can be a mess:
\begin{itemize}
    \item lots of tools
    \item Architecture dependent
    \item Language dependent
    \item Most programmers don't know about the architecture
\end{itemize}

\section{Experimental Methodology}
\label{sec:expe-methodo}

While in other domains such as biology experiments takes a large amount of
time and money, they are almost inexpensive and can be designed very quickly
in computer science. As a results, we computer scientists, are not used to
take the time to design correct experiments trying to minimize bias and using
the right tools. Furthermore as hardware and software evolves very quickly we
usually does not bother to re run and verify the results presented previous
studies while it is the best way to spot bias or confirm the validity of an
experimental claim.

In this section we first introduce why reproducibility matters and how people
have try to reach it in \gls{hpc}, then we present the methodology we have
developed during this thesis to make our experiment as reproducible as
possible.


\subsection{Reproducible research}

Measurement bias is a widely known phenomena in scientific communities and is
analyzed in most fields. Still in \gls{hpc} the only thing we usually do to avoid it
is to run a large number of experiments hopping that if we have enough
observations on several configurations the measurement bias will be
negligible. Mytkowicz et al.~\cite{Mytkowicz09Producing} highlighted several
way to introduce significant measurement bias in computer science experiments
without noticing it, showing that measurement bias is commonplace and
unpredictable in our field.

As measurement bias is unpredictable even when we follow the best practices
the easiest way to deal with it is to reproduce the study published by other
team and confirm or invalidate their results. While this is done in every
scientific fields, it is not common to publish about experiment reproduction
in \gls{hpc}.

A previous study~\cite{Collberg15Repeatability} tried to evaluate how
reproducible the experiment presented in computer science article are. To do
so, they focused on the capacity to build the experimental code and evaluated
$601$ articles published in “top ACM conferences and journal”. From these
$601$ articles they were only able to build the environment of $217$ articles.
Moreover it took more than half an hour to build $64$ of these papers and $23$
other required the intervention of the authors.

At this point we need to define precisely reproducibility, for the remaining
of this thesis, except if specified otherwise, we will use the definition
proposed by Dror G. Feitelson~\cite{Feitelson15From}:

\begin{quote}
    Repeatability concerns the exact repetition of an experiment, using the
    same experimental apparatus, and under the same conditions.

    Reproducibility is the reproduction of the gist of an experiment:
    implementing the same general idea, in a similar setting, with newly
    created appropriate experimental apparatus.
\end{quote}

It is nearly impossible to repeat an experiment in the exact same conditions
as many exterior factors, such as the machine room temperature or the network
usage done by other users  \DBm{Citations ? Demander a Arnaud ?} on the same cluster, might impact the measured
performances.
Still it is possible to repeat an experiment in similar conditions: someone
who runs the same experiments two different days on the same machines can
expect to obtain similar results. Several tools can helps use making
experiments more repeatable for instance by running our experiment on a shared
platform such as grid5000~\cite{Cappello05Grid5000} we can hope that other
people might access to the same set of machines. Moreover deploying a custom
environment on these machines allow to control and distribute the set of
installed libraries. Kameleon~\cite{Ruiz15Reconstructable} go even
further as it allows to describe an environment as a recipe thus makes it easy
to check the version of a library or replace it.

To reproduce an experiment it is important to understand how it has been
designed and how it evolves from the first version to the results presented in
the paper. Stanisic et al.~\cite[Chapter~4, p31-44]{Stanisic15Reproducible}
described an experimental workflow based on \emph{git} and \emph{emacs orgmode}
to keep track of these evolutions and make easy for someone to understand it.
One of the main drawback of this workflow is that it is not suitable for
experiment which generate huge ($\ge$\SI{500}{Mib}) trace files as git is not
designed to handle such files.

Several tools were designed to conduct experiments in computer science still
they are not designed for \gls{hpc} and using them in our context would
require some adjustments. A more complete survey can be found
in~\cite[Chapter~3, p17-19]{Stanisic15Reproducible}

\subsection{Experimental workflow}

We design experiments to answer simple questions such as ``Is my tool more
efficient than the existing ones'', obviously this formulation is too vague to
be answered scientifically. We first need to find a set of relevant state of
the art tools and then decide and on which configurations we are going to do
the comparison. This means choosing a set of benchmarks representing real life
applications, one or several credible experimental machine(s). Finally we must
determine a set of metrics that we can measured accurately to evaluate the
efficiency of the tools.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{tikz/exp-tikz}
    \caption{Experimental pipeline}
    \label{fig:exp-pipeline}
\end{figure}

All these choices form a high level \emph{experimental plan}: it contains all
the information required to reproduce the experiment but not to repeat it.
Indeed it miss the code that actually execute the experimental plan.
We can see an experiment as a pipeline that flow from this plan to the final
human readable results as described in \fig{exp-pipeline}. This pipeline is
composed of three steps: the experimental plan that produce a (usually) huge
set of data and meta data, the first one are meant to be used during the
analysis while the second are only generate to find issue in the experiment or
additional information to repeat it. Then comes the raw analysis that extract
pertinent information and compute metrics from the raw data. The results of
this step are usually easy to parse for a computer but not human friendly.
Thus the last step is there to present these data in a readable way.

Yet it is easy to present data in a tendentious way that reflects more our
expectations than the reality without even noticing it, we call this the
\emph{experiment designer bias}\DBm{Ref ?}. To limit this bias,
it is very important to design carefully the whole analysis before running the
actual experiment and test the visualisation on a filler set of data.
Furthermore, adding comments to the visualisations about what they mean and
what we expect to see can help avoiding this bias. Technologies such as
\texttt{R-markdown},\texttt{Jupyter Notebook} (anciently known as IPython
Notebook) or \texttt{Emacs Orgmode} are great help to produce reproducible
experiments.

\subsubsection{Methodology}

\paragraph{Preparing an experiment:}

In this thesis, we ran two types of experiments: the fist one aims at
analysing the behavior of one application (mostly \Gls{sofa}) while the second
one aims at comparing trace collection tools. For both analysis we need to
find a set of \emph{benchmarks} conduct our experiments, but the signification
of this term is slightly different for the two cases. In the first case, a
benchmark is an input for the studied application and the set of benchmarks
used should be defined with the developers of the application to cover the
code that they want to improve. While in the second case it means a set of
applications, for this need, we use the \gls{npb}~\cite{Jin1999} which allow
to cover a large set of applications from simple computational kernels to real
life applications. Yet to compare several analysis tools, a set of benchmarks
is not a sufficient input as these tools usually provides a large set of
parameters allowing to modify their behavior, thus we must evaluate each tool
under at least two set of parameters: the default and a tuned version with a
set of parameters optimized for the results we are measuring.

Once the benchmarks are chosen, we need to decide of one machine or a set of
machines on which we will run the experiment. This step is crucial as
computers architectures are getting more and more complex \DB{Ref partie 2.1
?} and applications are (usually) designed for one type of machines.
Furthermore some tools (such as \gls{Pin}\DBm{Citation ?}, \gls{PEBS}, \gls{IBS}) are vendor
specific or optimized for some architecture. Thus we must find machines that
allow to do our study in a relevant situation and fair to the tools and
libraries used.

Finally we need to find some relevant metric(s) to answer the question(s) that we
are trying to answer with our experiment. These metrics should remain as
simple as they will be interpreted by humans. Yet they must also cover every
aspects that we are studying. A complete set of simple measurable metrics is
often easier to understand that one complex metric that provide an overall
score in an unknown unit. Still simple ratios and percentages can make an
analysis more reproducible for instance an execution time is significant only
for a very specific configuration on one machine, while speedups and overheads
can be compared more easily.


\paragraph{Writing the experiment scripts:}

It is crucial that any step of the experiment from the deployment of the
environment to the final analysis is properly scripted in a language that can
be understood by an other developer. Any manual step can make the experiment
hard to reproduce and an obscure code might make it hard to change some
parameters.

Furthermore the traces generated by an experiment must be self explanatory or
at some point they will only be a (large) set of meaningless bytes on a hard
drive. To do so it is important to keep together the data and both the scripts
that generated it and the one that allows to interpret its. To do so, all our
experimental scripts starts the same way: they create a new directory that
will hold the traces, copy themselves and all the scripts which are on their
directory inside it. Then they duplicates their output to a file in this
experimental directory and Log every sensible meta data such as the
command that started the experiment, the machine name, its topology, \gls{OS}
the environment variables, the status of every dependencies (git version, diff
etc.). Such informations can be a great help to someone trying to reproduce
the experiment or to verify that the experimental condition are not biased.

The actual execution of the experiment consist of a set of runs, to limit the
impact of external noise, it is a good practise to randomize these runs.
Therefor our experimental scripts generate the list of runs that should be
executed shuffle it and then execute it run in a randomized order. A run
consist of three step: a pre command that can set specific values for the run,
the actual command that execute the benchmark and the post command that can
save data, might compute metrics and must restore the normal state. Note that
the first and last steps are not mandatory. Each of the commands executed by
these 3 steps are printed before execution thus logged.

At the end of the day our experiment have produced a huge set of data that we
can divide on two categories: the meta data that are useful for debugging and
reproducibility and the raw results that will be filtered to then be analyzed.
The parsing step is designed only to extract data from the traces so the
analysis scripts can work on clean results, we avoid to do any statistic
during this step.

Finally for the analysis \texttt{R} provides a large set of reliable statistic
analysis libraries and technologies such as \texttt{R-markdown} or
\texttt{Orgmode} allows to mix analysis code and results with comments. These
technologies allow to produce a standalone result (as a pdf or as a webpage)
that contains the original questions, our assumptions, the results and plots,
and our observation on these results.

\paragraph{Distributing the experiment:}

Trying to compile a badly documented code can be painful and time consuming,
the same problems occurs when it comes to reproduce experiment. Thus the
distribution of the experiment traces and script must be considered carefully.
To reproduce an experiment with need the scripts that run it, a description of
the machines and/or the virtual environment that is deployed and a link to the
right version of each tool, applications and benchmarks used during the
experiment. Several services such as Github, Bitbucked, and Gitlab \DBm{Urls ?}
host freely code repositories, furthermore \glspl{VCS} such as Git and
Mercurial \DBm{Citations ?} allow to track easily dependencies with a system
of submodules, therefore they are well suited for experiment distribution.
Finally an effort must be done on the documentation to help the user
understand why the experiment is designed, how it works and where (on which
machines) we should run it.

Distributing the experiment results can be more tricky as these files can be
quite heavy and code repositories are usually not designed to handle large
files. While the experimental repository allows to reproduce the whole
experiment, the trace distribution is useful to reproduce only some specific
steps of the analysis, thus we distribute to set of files corresponding to the
two analysis steps depicted in \fig{exp-pipeline}. The first one contains all
the files generated by the experiment and the scripts to run that analysis.
This distribution can be quite heavy (around \SI{100}{Gib} for some of our
experiments) but allows to redo the whole analysis, compute new metrics or
look for additional informations in the meta data. At the opposite, the second
one only contains the filtered results and the scripts that generate the
plots, it allows to redo the statistical analysis exploring the data with a
different approach. Furthermore this trace usually contains more plots and
data than a published article that have to fit a certain amount of pages. Such
traces can be distributed using specialized large file hosting services such
as Renater File Sender, or Zenodo.\DBm{Url, Citation ?}


\section{Sofa: case study}
\label{sec:sofa}

\subsection{Sofa}

\DB{Sofa particularities, different improvements approach (Everton / Julio)}

\subsection{Experimental analysis}


Voir Wiki/Meetings/CR\_Reu\_Sofa\_20140326.wiki

\DB{Application of the methodology presented above}

\subsection{Results and Discussions}

\DB{Need memory oriented tools, improve the methodology}


\subsection{Proposition}

\DB{Main goals of a memory analysis tool}


