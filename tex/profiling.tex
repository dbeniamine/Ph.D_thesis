%!TEX encoding=UTF-8 Unicode
\chapter{Performances analysis}

\section{Modern computing systems}

\subsection{Architectural considerations}

\DB{Caches, prefetechrs, NUMA few words about GPGPU, MIC \ldots}

\subsection{Usual performances issues}

\DB{All to all sharing, bad Mappings, false sharing \ldots}


\section{Profiling tools}

\DB{Main Biblio}

\subsection{Performance counters}

\DB{perfctr}

\subsection{Generic tools}

\DB{HPC toolkit etc}

\subsection{Discussion}

\section{Experimental Methodology}

While in other domains such as biology experiments takes a large amount of
time and money, they are almost inexpensive and can be designed very quickly
in computer science. As a results, we computer scientists, are not used to
take the time to design correct experiments trying to minimize bias and using
the right tools. Furthermore as hardware and software evolves very quickly we
usually does not bother to re run and verify the results presented previous
studies while it is the best way to spot bias or confirm the validity of an
experimental claim.

In this section we first introduce why reproducibility matters and how people
have try to reach it in \HPC, then we present the methodology we have
developed during this thesis to make our experiment as reproducible as
possible.


\subsection{Reproducible research}

Measurement bias is a widely known phenomena in scientific communities and is
analyzed in most fields. Still in \HPC the only thing we usually do to avoid it
is to run a large number of experiments hopping that if we have enough
observations on several configurations the measurement bias will be
negligible. Mytkowicz et al.~\cite{Mytkowicz09Producing} highlighted several
way to introduce significant measurement bias in computer science experiments
without noticing it, showing that measurement bias is commonplace and
unpredictable in our field.

As measurement bias is unpredictable even when we follow the best practices
the easiest way to deal with it is to reproduce the study published by other
team and confirm or invalidate their results. While this is done in every
scientific fields, it is not common to publish about experiment reproduction
in \HPC.

A previous study~\cite{Collberg15Repeatability} tried to evaluate how
reproducible the experiment presented in computer science article are. To do
so, they focused on the capacity to build the experimental code and evaluated
$601$ articles published in “top ACM conferences and journal”. From these
$601$ articles they were only able to build the environment of $217$ articles.
Moreover it took more than half an hour to build $64$ of these papers and $23$
other required the intervention of the authors.

At this point we need to define precisely reproducibility, for the remaining
of this thesis, except if specified otherwise, we will use the definition
proposed by Dror G. Feitelson~\cite{Feitelson15From}:

\begin{quotation}
    Repeatability concerns the exact repetition of an experiment, using the
    same experimental apparatus, and under the same conditions.

    Reproducibility is the reproduction of the gist of an experiment:
    implementing the same general idea, in a similar setting, with newly
    created appropriate experimental apparatus.
\end{quotation}


\begin{itemize}
    \item Repeatability
        \begin{itemize}
            \item Not obvious but reachable
            \item Need access to same machine
                \begin{itemize}
                    \item Virtualization
                    \item Shared platform grid5000~\cite{Cappello05Grid5000}
                    \item Kameleon recipe~\cite{Ruiz15Reconstructable}
                \end{itemize}
            \item List dependencies
            \item Correct documentation
        \end{itemize}
    \item Reproducibility
        \begin{itemize}
            \item State of the art see \cite[Chapter~3, p17-19]{Stanisic15Reproducible}
            \item Luka~\cite[Chapter~4, p31-44]{Stanisic15Reproducible}: make whole experiment feedback
                loop reproducible. Based on git orgmode, not suitable for analysis
                generating huge traces
            \item Importance of logs
        \end{itemize}
\end{itemize}

\DB{More biblio on existing tools}


\subsection{Experimental workflow}

\begin{figure}[htb]
    \centering
    \includegraphics[height=.96\textheight]{tikz/exp-tikz}
    \caption{Experimental workflow}
    \label{fig:exp}
\end{figure}

\begin{itemize}
    \item Focus on experiment and analysis reproducibility (not
        feedback/experiment loop)
    \item See experiment as a workflow described in \fig{exp}
    \item Goal: Make all the workflow repetable or at least reproducible
        \begin{enumerate}
            \item Setup
                \begin{itemize}
                    \item Make a plan:
                        \begin{itemize}
                            \item Case study (benchmarks: application representing real life
                                problems).
                            \item Configurations (different version of the benchmarks code,
                                runtimes with different set of parameters).
                            \item Experimental machines / environment (recent enough,
                                representative AMD / Intel ?, several OS versions ?)
                        \end{itemize}
                    \item Prepare analysis:
                        \begin{itemize}
                            \item What do we want to know ? Which metrics can answer our
                                questions ?
                            \item Avoid execution time, prefer overhead, speedup, slowdown
                        \end{itemize}
                    \item Store everything in a git repository
                    \item Use git submodule for dependencies
                    \item An init script can apply patches if required
                    \item Launch scripts responsible for the experiment
                        deployment should be included
                \end{itemize}
            \item Raw analysis
                \begin{itemize}
                    \item Simpler step but crucial
                    \item Extract important information from raw traces
                    \item Do not compute thing here
                    \item Heavy files cannot be in a git repository
                    \item Use online storage (for instance zenodo)
                    \item A Readme should explain file hierarchy
                    \item Include filtering and analysis scripts
                \end{itemize}
            \item Statistic analysis
                \begin{itemize}
                    \item Mix text with code (R-markdown, orgmode \ldots)
                    \item Prepare it before run
                    \item Repeat original question and expected results
                    \item Use online storage (for instance zenodo)
                    \item A Readme should explain file hierarchy
                    \item Include analysis scripts
                \end{itemize}
            \item And now ?
                \begin{itemize}
                    \item Raise more question => new experiments
                    \item Enough answers => write paper
                \end{itemize}
        \end{enumerate}
\end{itemize}

Actual experiment (on the experimental machine)
\begin{enumerate}
    \item Log every thing
        \begin{itemize}
            \item Machine (name, topology, OS \ldots, OS
                config)
            \item Dependencies status (git version, diff etc.)
                script diff
            \item Environment variables, script arguments etc.
        \end{itemize}
    \item Preparation
        \begin{itemize}
            \item Clean then compile every thing
            \item Set environment variables
            \item Prepare ordrer of runs (if randomization)
        \end{itemize}
    \item Execute Runs:
        \begin{enumerate}
            \item PreCommand: set specific values for the run
            \item Run: do the experiment
            \item PostCommand: Save data, compute metrics,
                restorenormal state
        \end{enumerate}
        precommand and postcommand must cancel each other
\end{enumerate}

\DB{My description of experimental workflow / how to make it more
reproducible}

\section{Sofa: case study}

\subsection{Sofa}

\DB{Sofa particularities, different improvements approach (Everton / Julio)}

\subsection{Experimental analysis}

\DB{Application of the methodology presented above}

\subsection{Results and Discussions}

\DB{Need memory oriented tools, improve the methodology}


\subsection{Proposition}

\DB{Main goals of a memory analysis tool}
