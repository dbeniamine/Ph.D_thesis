%!TEX encoding=UTF-8 Unicode
\chapter{Introduction}

% De tout temps les homes
Scientists from all fields use computers to ease their calculations and run simulations to test their hypothesis.
These simulations are more and more complex as science advances and therefore requires always more computing power.
In a first time, computer vendors increased this power by increasing the frequency of their \glspl{CPU}, but this approach reached quickly several hard physical limits.
To overpass them, they started to build parallel processors.

% Hard limits
The first limit comes from the energy required to increase the frequency of a \gls{CPU}.
Indeed, according to \gls{Intel}~\cite{Ganesan16Introduction}, over-clocking a processors by \SI{20}{\%} only increase the performances by a factor $1.13$ but requires $1.73$ times more energy.
At the opposite, using an identical processor with two cores instead of one with a frequency \SI{20}{\%} lower provides $1.73$ times more performances for only $1.02$ times more energy.
The second limit is the speed of light: data have to travel from memory to the \gls{CPU} and cannot go faster than the speed of light.
While this limit can seem high, we have already reached it.
Indeed, if we want to build a sequential machine able to process \SI{1}{TByte} of data per second, due to this limit it would require  to stick \SI{1}{TByte} of data on an area of \SI{0.3}{mm^2} which means that $1$ bit occupies only \SI{0.1}{nm}, the size of a small atom.

% Multi socket
While these parallel processors are theoretically more powerful than sequential ones, it is way more complex to use them efficiently and it is the responsibility of the developer to do so.
Moreover since a few years we are reaching yet another physical limitation.
We are capable of reducing the size of transistors, hence increasing the number of transistors in a chip.
Still, more transistors means more heat, and there is a maximum of heat that an area can produce before unexpected effects such as leakage occurs.
As a result, vendors are now building machines with several sockets, each one embedding several cores.

% Gap CPU / memory => caches
At the same time, processors became significantly faster than memory, thus, \glspl{CPU} embed small caches memory to limit the impact of this gap on performances.
Theses caches are designed to benefit from two patterns that occurs in most programs: spacial and temporal locality, which respectively means using data close in memory and using several times the same data in a short time lapse.
One of the things that makes the caches faster than the main memory is their size, the smaller they are, the faster it is to access them.
Therefore \glspl{CPU} embed several level of caches (usually three), the first level is very fast and small few kilo bytes and designed for very close accesses (loop on an array), while the  last level is bigger and slower, about \SI{10}{MBytes} and designed for more distant accesses.
As the \glspl{CPU} embed several cores, theses caches are organized hierarchically, the last level is shared by all cores while each core has a private access to a level one cache.
This hierarchy helps isolating private data and benefit from well structured sharing, moreover it reduces the required bandwidth in the caches.
For similar reasons, computers with several sockets have a \gls{NUMA} which means that each socket has a privileged access to a subpart of the memory.
Consequently, the memory access patterns of an application can significantly impact its performances~\cite{Drepper07What}.
Indeed, four threads working on small and separate piece of data will benefit from their private caches while patterns such as all to all sharing will result in a lot of conflicts in the caches.
Moreover, if some sharing occurs between threads that are close in this hierarchy, the shared caches will contain shared data and one thread will benefit from the accesses of the other.
At the opposite, if theses threads are far in the hierarchy, the sharing will generate some noise and maybe some contention on the memory bus.

% Need for perf analysis
In the end, writing an efficient program requires to consider the architecture of the computer that will run it, and the patterns, and their matching, even if the program is sequential.
Although there are some general rules: privileging sequential accesses, working on small set of data, this task is extremely complex, even for \gls{HPC} specialists, as every accesses matters.
Thus, performance analysis tools are extremely helpful to understand and optimize the performances of any application.

% Classic tool not good for memory related issue
The first step to optimize the performances of an application is to find the hotspots, which means the parts of code that are inefficient and understand their nature.
Only at this point it is possible to decide what part of code should be improved and how.
There are many tools designed to analyze the performances of an application~\cite{Pillet95PARAVER,Browne00Portable,Shende06Tau,Treibig10LIKWID,Adhianto10HPCTOOLKIT} most of them rely on performance counters to collect a trace of the application.
These are \gls{CPU} register dedicated to performance analysis which enable efficient collection of performance data.

In this thesis we ran a case study on the performance analysis of a physical simulation framework: \gls{SOFA}.
To analyze the performances of this application, we used \gls{Likwid}~\cite{Treibig10LIKWID}, a classical performance analysis tool and traced several metrics concerning the memory usage.
With this tool we are able to detect memory related performance issues and guess the nature of some of them, but it was impossible to spot their location on the memory and the patterns responsible for the bad performances.
Indeed, if these counters can be very useful, they consider the memory as a monolithic entity which is not the case on recent architecture.
Thus, specific tools should be used for analyzing performances in view of memory.

% Memory analysis is complex
Analyzing an application performances in view of the memory raises two challenges: the first one is the collection of the trace itself.
This is a complex task as there is no hardware comparable to the performance counters for tracing memory accesses.
Furthermore, every instructions of a program triggers at least one memory access, thus, collecting every single memory accesses of an application is not possible.
Additionally, due to the lack of tracing hardware, a memory collection tool might easily become invasive and significantly change the behavior of the analyzed application.
The second challenge is the presentation of the trace.
Indeed, memory traces are extremely complex as they are spread over five dimensions: time, address space, \gls{CPU} location, threads and access type.
Furthermore, some of these dimensions are not trivial to represent, for instance the address space can be virtual or physical and the \gls{CPU} location is organized hierarchically.
In the end, memory analysis tools have to extract pertinent data and present them in an understandable way.

% Perfect memory tool
An ideal memory analysis tool should be able to present the memory access patterns of a program to its developer, including information about data sharing between threads and the location of the access on the machine architecture.
Furthermore, such tool should highlight inefficient patterns.

% Existing tools
Several tools were designed for memory performance analysis~\cite{Lachaize12MemProf,Liu14Tool,Gimenez14Dissecting}, however most of them addresses the trace collection challenge by doing an instruction sampling.
Instruction sampling is a hardware based technique that enable tracing some instructions at a defined frequency~\cite{Drongowski07Instructionbased,Levinthal09Performance}.
While this method enables efficient tracing, it does not trace the whole memory space addressed.
As a result, it is impossible to visualize memory patterns from the collected trace.

\section{Contributions}

% Proposition
In this thesis we propose two tools to analyze the memory behavior of an application.
Our first tool, called \gls{Tabarnac}, collects global memory traces without temporal information and presents an overview of the sharing patterns inside the data structures, between the threads of the execution.
The second one, called \gls{Moca}, collects generic, fine grained  memory traces with temporal information.
We propose two approaches to visualize \gls{Moca} traces, the first one is based on \gls{Framesoc} an existing generic trace analysis framework, while the second one relies on a programmatic exploration using \gls{R}.

% Experiments
Conducting experiments in computer science can be extremely simple, but doing it in a reproducible way requires more planning and methodology.
Performance analysis, whether it is for optimizing an application or to evaluate a tool requires to do complete experimental campaigns.
In this thesis we, we take a particular attention at making our experiments as reproducible as possible.
To do so, we clearly describe our experimental methodology and distribute the files required to reproduce each step of the presented experiments.

\subsection{Global overview of the memory sharing patterns}

We designed \gls{Tabarnac} to analyze the memory sharing patterns of applications running on \gls{NUMA} machines.
This tool relies on an existing, lightweight binary instrumentation, based on the \gls{Intel} \gls{Pin} library, which counts how much each thread of an application accesses each page.
We added to this library the capacity to retrieve contextual information to associate memory addresses to data structures (static and allocated).
Moreover we designed several comprehensive visualizations of the collected traces.
Using these visualizations we where able to identify some performance issues and improve the performances of a well known benchmark by \SI{20}{\%}.

These results were published in an article at \gls{VPA} 2015 a Super Computing workshop~\cite{Beniamine15TABARNAC}.
Furthermore \gls{Tabarnac} is distributed as a free software under the \gls{GPL}: \url{https://github.com/dbeniamine/Tabarnac}.
This work is the result of a collaboration with M. Diener and P.O.A Navaux from the informatica team of the \gls{UFRGS}, Porto Alegre, Brazil, financed by CAMPUS France.

\subsection{Fine grain memory traces collection}

\gls{Moca} is our main contribution.
This tool relies on a \gls{Linux} kernel module to collect efficiently fine grain memory traces.
This kernel module intercepts page faults, which are triggered by the hardware and handled by the \gls{OS}, to trace memory accesses.
As these page faults does not occurs frequently, it also injects periodically false page faults.
It handles memory traces in the kernel space and flush them to userspace periodically.
Moreover we incorporated our data structures tracking library in \gls{Moca} without the dependency to \gls{Pin}.
Additionally we ran an extensive experimental comparison of \gls{Moca} comparing it to \gls{Tabarnac} and two state of the art memory analysis tools in terms of overhead, trace precision and completeness.

This work is the subject of two Inria research reports~\cite{Beniamine15Memory,Beniamine16Moca} and has been submitted at \gls{CCGRID} 2017.
As the previous tool, \gls{Moca} is distributed under the \gls{GPL} license: \url{https://github.com/dbeniamine/Moca}.

\subsection{Fine grain memory traces analysis}

We proposed two different approaches to visualize \gls{Moca} traces.
The first one is based on an existing general trace management and analysis framework called \gls{Framesoc}~\cite{Pagano14frameSoC}.
More precisely, it relies on \gls{Ocelotl}~\cite{Dosimont14Ocelotl} a \gls{Framesoc} tool that aggregates similar parts of the trace and present a simplified overview highlighting anomalies.
With this tool we were able to identify classical inefficient memory patterns on a test application.
Nevertheless we encountered several scalability issues due to the generic representation of the trace inside the tool.

To overpass theses scalability issues, we also explored several \gls{Moca} traces with a programmatic approach using \gls{R}.
This approach enables using advanced filtering and zooms, and to design specific visualization for each traces.
With this method we were able to explore more complex traces and detect some interesting and unknown memory patterns.
For reproducibility, these analysis are saved and versioned in a labbook publicly available at github:\\
\url{https://github.com/dbeniamine/Moca_visualization}.

\section{Thesis organization}

The remaining of this thesis is organized as follow:
in \chap{perf} we present a case study on the performance analysis of \gls{SOFA}, a physical simulation tool.
This chapter first introduces \gls{SOFA}, its specificities and previous attempts to optimize it, and highlight the need for performances analysis on \gls{SOFA}.
We then discuss the existing generic performance analysis tools and our experimental methodology.
This case study emphasize the need for specific memory performance analysis tools.
In \chap{mem}, we introduce some specificities of recent memory architectures, usual memory performance issues and workarounds.
Then we discuss the existing memory performance analysis tools, their limitations and what we would expect from an ideal tool.
After that we present \gls{Tabarnac}, our first contribution, in \chap{tabarnac}.
We discuss its design and usage, evaluate its overhead and finally present some performance optimization done with the knowledge obtained thanks to \gls{Tabarnac}.
In \chap{moca}, we describe our main contribution, \gls{Moca}.
We first explain in details the mechanisms used by \gls{Moca}, its internal design and how it handles the challenges raised by fine grain memory trace collection.
Then we provide an extensive experimental evaluation comparing \gls{Moca} to two state of the art memory analysis tools and \gls{Tabarnac}.
\chap{analyzing} discusses the visualization of \gls{Moca} traces.
We first introduce \gls{Framesoc} and \gls{Ocelotl} and the discuss the results obtained with these tools.
Then we propose a programmatic approach and present the visualizations and results obtained with it.
Finally we draw our conclusions and present some perspectives of future work in \chap{cncl}

\glsresetall
% vim: et si sta lbr  sw=4 ts=4 spelllang=en_us
