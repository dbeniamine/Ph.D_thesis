%!TEX encoding=UTF-8 Unicode
\chapter{Memory Performance Analysis}
\label{chap:mem}

The results of our case study (\chap{perf}) showed that traditional performance analysis tools can help identify memory related performance issues.
Yet they are not able to tell precisely where, in terms of data structures, the issue occurs.
Thus it is still required to analyze the code manually.
As memory is often a performance bottleneck, several tools where developed to analyze performance in regards of the memory.
However, none of these tools is currently able to present the memory access patterns of an application.

This chapter discuss memory analysis tools, first we present the specificities of recent memory subsystems and the usual mistakes that can generate performance drop in \sect{archi}.
Then we present existing memory performance analysis tools, how they relate to usual memory issues and discuss their limitation in \sect{mem-tools}.
Finally we describe what would be in idealistic memory performance analysis tool \sect{mem-cncl}.

\section{Architectural considerations}
\label{sec:archi}

During a few decades, processor frequency increased significantly more than memory frequencies, resulting in a considerable gap between these two resources.
Nevertheless most applications use spatially close data (spatial locality) and temporally close data (temporal locality).
To benefit from these localities, \glspl{CPU} embed some caches reducing the number of accesses actually generating a fetch from the main memory.

\subsection{Caches}

% Locality
There are two phenomena called \emph{spatial locality} and \emph{temporal locality} that can be observed in most programs and that are the main principles used for cache optimization.
Spatial locality is the fact that most programs do not usually access the memory in a completely random way, they usually access data that are near to each other in a small time lapse.
Similarly, temporal locality is the fact that a memory address is often used several times during a small interval of time.
Consequently the main role of caches is to keep data that have already been accessed close to the \gls{CPU} when they are reused to reduce the number of actual memory accesses.

These caches are organized hierarchically, thus, the time required to access a piece of data depends on the cache level in which it is present.
One access to the fastest cache usually cost about $4$ \gls{CPU} cycles while retrieving one piece of data from the main memory cost around $180$ \gls{CPU} cycles~\cite{Levinthal09Performance}.
As these caches are small, each time a data is added in a cache it replaces (evict) an existing one.
Several mechanisms where designed to guess which data should be in the cache and which one should be evicted.
As a result, a developer seeking for performance must consider the architecture of these caches and the way the work to benefit from them.

\subsubsection{Cache lines and alignment}

To benefit from spatial locality, the memory is divided on lines (usually \SI{64}{Bytes}), and every time an address must be fetched from the main memory, the whole line is copied to the cache.
Therefore accessing $8$ successive doubles (usually one double equals \SI{8}{Bytes}) requires only one memory transaction if they all are in the same line of cache.
As a result, aligning data structure to cache line can improve the memory access time.

For instance if we consider accessing an array of two lines of caches as illustrated in \fig{bad-align}.
If the data structure is aligned to cache lines, two accesses will be required to retrieve the whole data structure, and no unused data will be introduced in the cache.
At the opposite, if it is not correctly aligned, not only one more memory fetch will be required but one useless line will be introduced inside the cache.
As caches are designed to keep only a limited volume of data, inserting one line inside a cache means evicting another one, thus, introducing unused data must be avoided as much as possible.

\begin{figure}[htb]
    \centering
    \input{tikz/bad-align}
    \caption[Example of bad alignment.]{Retrieving two lines of cache with one or two fetches depending on the alignment of the lines.}
    \label{fig:bad-align}
\end{figure}

\subsubsection{Cache management policies}

When an address is accessed, the \gls{CPU} must search for the corresponding line in the cache.
If it is not present, the \gls{CPU} must fetch it from the memory and decide where to copy it into the cache, which means choosing one line to evict.
To make the cache efficient, these decisions must be done quickly.

There are three distinct ways to decide where a line should be placed in the cache.
The simplest way to do so is called \emph{direct mapping} and consists in associating each memory line to one specific line of cache in a round robin way: the line number $l$ can only be in the cache line $l \mod L$ where $L$ is the total number of available lines in the cache.
With this policy it is only required to look at one line of cache to check whether a line of memory is present or not in the cache.
Furthermore, it is not required to decide which line should be evicted as their is only one possible line.
Still, this policy is inefficient with some memory patterns, for instance when a program accesses data regularly spaced but misaligned in the memory.
Indeed, for such pattern, only a small subset of the cache will be used.
At the opposite, \emph{fully associative caches} allows any memory line to be mapped anywhere in the cache.
As a result we always exploit the maximum size of the cache, but it requires to look at the whole cache to find if a line is present, and deciding which line should be evicted.
Usually caches are \emph{N-way associative} which is a compromise between those two policies.
A N-way associative cache is divided in $N$ sets and the memory line number $l$ could be in any line of the set number $l \mod N$.

This associativity can be used to create (virtual) partitions in the cache and then allocate data structures in these partition giving more cache to the ones that are more reused and thus will benefit from it~\cite{Perarnau11Controlling}.
However, this technique is not possible anymore with recent \gls{Intel} processors as they change dynamically the associative sets.

Choosing the best line to evict is impossible as it would require to predict the future steps of the execution.
A realisable and often efficient policy consists in evicting the \gls{LRU} line that could be replaced by the one fetched.
Nevertheless, implementing an actual \gls{LRU} policy is costly as it requires to timestamp every line inside the cache.
Consequently most caches use pseudo-\gls{LRU} heuristics.

\subsubsection{A naive example}

The naive matrix multiplication is a good illustration of how a simple program can benefit from the caches or not.
\fig{mat-mult} illustrate a naive, sequential matrix multiplication algorithm that computes $C=A*B$.
For the matrices $A$ and $C$, this algorithm loops over a whole row before going to the next one, while it goes through $B$ by columns first.
To understand why this pattern matters, we have to consider the memory representation of these matrices.
As the memory address space has only one dimension, a matrix is a contiguous block of memory.
Usually it is stored row major, which means that $A[i][j]$ is actually $A[i*N+j]$ where $N$ is the size of a row.
This means that, when we loop through a matrix by rows first, we scan linearly the address space, while if we do it by columns first, we jump $N$ elements between two accesses.
In this example we use $N=8192$, the first access to a row of $B$ will trigger a cache miss, and the whole cache line will be fetched.
Before we access the second element of this row, we will have to fetch one line of cache for each row of the matrix which means $8192*64=512$Kb which is a little more than the size that the L2 cache can accommodate.
Therefore each access to $B$ will trigger at least a L2 cache miss resulting in a lots of traffic in the L3 cache.
With huge matrices, it may not even fit in the L3 cache resulting in contention on the memory bus.
The simplest way to fix this issue (although it is not the optimal algorithm for the matrix multiplication) consists in swapping the two inner loop of the algorithm, as shown by the dotted arrows.
Indeed the order of the operations does not change the result of the multiplication (all computations are independent in this example) this swap only changes the order of the accesses concerning the matrix $B$.

\begin{figure}[htb]
    \centering
    \input{tikz/mat-mult}
    \caption[Example of non linear memory accesses.]{Example of non linear memory accesses: the naive matrix multiplication.}
    \label{fig:mat-mult}
\end{figure}

Recent caches also embed a prefecther that tries to detect memory accesses patterns to retrieve several lines of cache at the same time from the main memory.
This mechanism is particularly efficient with linear or regularly spaced accesses.
Yet, for sparse and random accesses it might prefetch unused lines evicting potentially useful ones.
Consequently, this mechanism amplify the impact of the accesses regularity.

\subsubsection{Memory caches and parallelism}

Several reasons have led vendors to make their \glspl{CPU} more and more parallel.
For instance, increasing by \SI{20}{\%} the frequency of a \gls{CPU} requires much more energy than using two cores at the original frequency and results in less computational power.
Furthermore, increasing the frequency of a processor increases also the amount of heat produced, and there is a physical limitation on the maximum heat that an area can produce before current leakage happens.
As a result, modern \glspl{CPU} embed several cores.

Using one huge private cache per core would be costly and space consuming, at the opposite if we use only one cache shared by each thread, they will interfere with each other all the time.
Therefore, the caches are organized hierarchically.
The highest level of cache is shared by each core while the lowest is private, the intermediate levels are usually shared by a subset of the cores.
\fig{topo-single} depicts a quad core machine with hyperthreading enabled (two threads per physical core).
This machine embed three levels of cache, the first one is private for each core, while the second one is shared by half the cores and the last one is shared by every cores.

\begin{figure}[htb]
    \centering
    \input{tikz/topo-single.tex}
    \caption{Topology of a quad core parallel machine.}
    \label{fig:topo-single}
\end{figure}

A consequence of that hierarchy is that the physical location of thread sharing data can have a considerable impact on the performance.
Indeed if two threads on the same core read the same line, the first one will copy it to the L1 cache and the second one will access it extremely fast.
If they are executed inside different cores, the sharing will be efficient at the lowest common level (L2 or L3).
At the opposite, if two threads write data in the same cache line, a conflict occurs and must be solved by the cache coherency protocol.
The coherency is done at the lowest shared level of cache and requires to lock the memory subsystem.
Therefore conflicts are extremely costly, and must be avoided when possible, or at least must occurs between threads as close as possible to avoid locking the L3 cache.

For instance, we  consider a simple example where two threads are working on a small array of $8$ doubles.
As a double is usually coded on $8$ Bytes, this array is exactly the size of one cache line.
Each thread is doing independent computations on a half of the array as illustrated in \fig{false-sharing}.
The first access will copy the whole array from the memory to all the caches levels of the thread that triggers it.
When the second thread reads the array, it will copy it from the lowest shared cache to its private cache.
If the threads were only reading it, no more memory access would be required.
Yet, in our example, each thread updates the value of each entry of its array after its computation.
Each time a thread writes a value of the array, it invalidates the whole line.
Therefore the coherency protocol must interfere at the lowest shared level, while the two threads are not actually sharing any data.
Hence the name false sharing.
Not only the accesses to this array are inefficient but they generates lots of useless data traffic in the memory bus which can create some contention slowing down the whole application.
The easiest way to fix such issues, consists in padding the data structure, which mean introducing zeroes between each element of the data structure so that each thread works on a different cache line.

\begin{figure}[htb]
    \centering
    \input{tikz/false-sharing}
    \caption[Example of false sharing.]{Two threads writing 4 consecutive doubles on the same line of cache, without any actual sharing, resulting in false sharing and an easy fix by padding the data structure.}
    \label{fig:false-sharing}
\end{figure}

\subsection{Memory hierarchy}

Increasing the parallelism inside a chip means fitting more transistors in a limited space.
Therefore, it requires to reduce the size of the transistors and find a way to dissipate the heat produced by them.
Consequently it is not possible to increase indefinitely this parallelism.
As a results, modern computers often embed several \glspl{CPU} sockets to overpass this limitation.
A machine with several sockets can either give them a uniform access to the memory by sharing the memory bus or split the memory into banks and giving non uniform access to the sockets, such machine is called \gls{NUMA}.
While the first option seems simpler to use, it means that the bandwidth is shared by all the threads, therefore contention can easily appear.
At the opposite, the second option provides a maximal bandwidth for each socket.
\fig{topo-NUMA} depicts a \gls{NUMA} machine, we can see that each socket has a privileged access to one memory bank.
Furthermore, the sockets are linked via a interconnect with a ring topology.
As a result, each socket  has a direct access to its memory bank, a slower one the banks of its neighbors and an even slower access to the last bank.
Writing code that uses efficiently this specific architecture remains the burden of the developer who therefore needs to explicitly consider the physical location of its data.
\tbl{mem-latency} provides approximate accesses latencies depending on the memory hierarchy level for  the \gls{Intel} I7 Xeon 5500 Series.

\begin{figure}[htb]
    \centering
    \input{tikz/topo-NUMA.tex}
    \caption{Topology of a 32 cores NUMA machine.}
    \label{fig:topo-NUMA}
\end{figure}

\begin{table}[htb]
    \centering
    \begin{tabular}{lrr}
        \toprule
        \textbf{Data source} & \textbf{Latency (cycles)} & \textbf{Latency (ns)}\\
        \midrule
        L1 Cache hit                            & \textbf{\textasciitilde4}          & \textasciitilde1.6\\
        L2 Cache hit                            & \textbf{\textasciitilde10}         & \textasciitilde4\\
        \midrule
        L3 Cache hit, unshared                  & \textbf{\textasciitilde40}         & \textasciitilde16\\
        L3 Cache hit, shared in other core      & \textbf{\textasciitilde65}         & \textasciitilde26\\
        L3 Cache hit, modified in other core    & \textbf{\textasciitilde75}         & \textasciitilde30\\
        Remote L3 Cache                         & \textbf{\textasciitilde100-300}    & \textasciitilde40-120\\
        \midrule
        Local Memory                            & \textasciitilde180                 & \textbf{\textasciitilde60} \\
        Remote Memory                           & \textasciitilde250                 & \textbf{\textasciitilde100} \\
        \bottomrule
    \end{tabular}
    \caption[Approximate access latency depending on the memory hierarchy level.]{Order of magnitude of access latency depending on the memory hierarchy level.
    Values in bold extracted from Levinthal report on performance analysis for \gls{Intel} I7 Xeon 5500 Series~\cite{Levinthal09Performance}.
    Conversion cycles to latency computed for a CPU frequency of 2.5GHz.
}
\label{tab:mem-latency}
\end{table}

To use \gls{NUMA} machines efficiently, we need to understand how the \gls{OS} handles the memory.
From the \gls{OS} point of view, the memory is split into contiguous chunks called pages, usually one page corresponds to \SI{4}{Kb} (some \gls{HPC} applications uses huge pages of \SI{4}{Gb}).
Each userspace program works on virtual pages, which means that, when it accesses an address, the \gls{OS} must first translate it to find the actual physical address of the page.
This pagination is used to provide the abstraction of virtual memory and the memory isolation.
\gls{Linux} is a lazy \gls{OS} thus it will never map a page until a program has written something to it.
Indeed if a program reads the contents of a new page, \gls{Linux} can simply return a zero.
To do so, one page full of zeroes is always present in the memory and any virtual page points to this specific page until a program write or explicitly touch it.
This means that the physical location of a piece of data is determined the first time that a program write something on the virtual page containing this data.
At this point, the \gls{OS} needs to decide where it is going to map the page.
One of the most classic and simplest policy, used by most \glspl{OS} is the first touch policy~\cite{Marchetti95Using}.
This policy maps a page to the memory bank closest to the socket where the thread responsible for the first access is executing.
As a result, the thread(s) initializing a data structure will determine its mapping.
A classic performance issue with \gls{NUMA} machines comes from the initialization of all the data structures using only one thread.
When doing so, all pages are mapped on the same memory bank, and each further access from another socket will be remote and, thus, slow.

% Manual mapping
Kleen et al. developed an interface to map the pages on \gls{NUMA} machines using more advanced policies than first touch~\cite{Kleen05NUMA}.
This \gls{API} can be accessed either via the \texttt{numactl} command or via a library called \texttt{libnuma}.
The \texttt{numactl} command is useful to apply a global policy on all the page of the application.
It is often used to apply the \emph{interleave} policy that distribute the pages over the \gls{NUMA} nodes in a round robin way.
While it does not reduce the overall number of remote accesses, it distribute them among the nodes and therefore reduces the contention when there are more than two sockets.
At the opposite the \texttt{libnuma} provides fine grain pages and threads mapping.
The user can use it to explicitly allocate data structures and bind threads to nodes.

% Adaptive
Still, finding the optimal mapping for one machine is not trivial and, mapping threads and data structures in an adaptive way is even harder.
Therefore, several tools were developed to automatically map threads and pages online~\cite{Diener14kMAF,Corbet12Toward}.
Such tools count remote accesses for each page, and move them when they are more accessed remotely than locally.
While adaptive tools often improve the performance, they cannot reach the same performance as an application optimized while considering memory issues.
Indeed, these tools, by conception, require a time to detect inefficient patterns and adapt and consequently lose opportunities of optimizations.

% First touch hack
An easy way to overpass this issue for small computational kernels consists in running a loop of computation on the data before initializing them.
Indeed, by doing so, each page will be mapped as close as possible to the first thread that will actually use it.
Nevertheless, this technique is only efficient for kernel that repeat several iterations of the same computations and is not suitable for more complex applications

To conclude, using efficiently the memory is challenging.
Indeed, due to the organization of the memory in pages and cache lines, we must consider where and how our data structures are allocated.
Furthermore the hierarchical organization of the memory and the cache must be correlated with the thread placement and data sharing.
In summary, the developer must consider the machine architecture and the memory access patterns over the address space, time and threads.
Therefore visualizing the memory access patterns of an application is a great help to optimize it.

\section{Existing tools}
\label{sec:mem-tools}

Presenting memory access patterns to the user raises two challenges.
The first one is to collect efficiently a detailed and precise trace without interfering with the normal execution.
Collecting such traces is challenging as each instruction of a program triggers at least one memory access.
Once this trace is collected, presenting it in a meaningful way to the user is also a challenge.
Indeed such traces are spread over at least five dimensions: memory address space (physical and virtual), time, threads, cores and type of accesses.
Furthermore they are potentially huge, and identifying relevant information is complex.

\subsection{Memory traces collection}

To help the developer solve memory related issues, an ideal tool should provide enough data to build a map of the memory accesses locations over the time, and identify memory accesses patterns such as false sharing.
Therefore the trace it collects must have the following properties:
\label{def:traces}
\begin{itemize}
    \item  \textbf{Detailed:}  a memory trace is \emph{detailed} if it includes information about time, space (at which address the event occurs), location (on which CPU it occurs) and nature of access (is this a read, a write, by which thread).
    \item \textbf{Precise:} to be \emph{precise} enough, a trace should include a sufficiently large number of events in order to enable identification of memory accesses patterns.
    \item \textbf{Complete:} We say that a trace is \emph{complete}, at a given granularity, if and only if the events it contains covers the whole address space at this granularity.
\end{itemize}

Several studies propose to analyze memory  by looking solely at the information collected through \gls{PAPI} and \gls{Likwid} libraries~\cite{Majo13(Mis)understanding, Jiang14Understanding,Bosch00Rivet,Weyers14Visualization,Tao01Visualizing,DeRose01Hardware}.
Generic tools have been designed on top of hardware performance counters to analyze and improve parallel applications performance, such as Intel's \gls{VTune}~\cite{Reinders05VTune}, \gls{PCM}~\cite{Wilhalm12Intel}, the \gls{HPCToolkit}~\cite{Adhianto10HPCTOOLKIT}, and AMD's \gls{CodeAnalyst}~\cite{Drongowski08introduction}.
Although performance counters provide information about the memory use (bandwith, volume of data transferred \ldots),  they consider the memory as one huge entity and do not differentiate distinct addresses or at least distinct pages.
Thus, these methods lacks of precision as they are not able to locate issues in the memory and determine in which data structure they happen.

Tracing all the memory accesses without information loss is nearly impossible as almost each instruction can trigger a memory access in addition to its fetch.
Nevertheless, several methods can record a \emph{detailed} memory trace with a good \emph{precision}.
Budanur et al.~\cite{Budanur11Memory} use an instrumentation based tool to collect all the memory accesses.
They loose \emph{precision} by doing online compression and merging accesses into a higher level model, but this is necessary to reduce both the trace size and its overhead.
Still, on a small matrix multiplication (size $48*48$ with four \gls{OpenMP} threads) they already slow the execution down by a factor $50$.
Another method consists in using hardware sampling tools such as AMD's \gls{IBS}~\cite{Drongowski07Instructionbased} or Intel \gls{PEBS}~\cite{Levinthal09Performance} to trace a subset of the memory accesses.
This method is used by many several tools, including the memory trace module of \gls{HPCToolkit}~\cite{Liu14Tool}, \gls{Memphis}~\cite{McCurdy10Memphis}, \gls{MemProf}~\cite{Lachaize12MemProf}, and \gls{Mitos}~\cite{Gimenez14Dissecting}.
This method provides \emph{incomplete} sampling: some parts of the memory can be accessed without being noticed by the tool if none of the associated instructions are part of the sampled instructions.
Thus, it is possible that they ignore memory areas less frequently accessed, but in which optimization could take place.
Applications sensitive to spurious performance degradation, such as interactive applications, could be hindered by these unnoticed accesses, despite their low frequency.
Furthermore, to be able to detect patterns such as false sharing, theses sampling mechanisms should be able to collect several samples every \SI{10}{cycles} which means around \SI{100}{ns}.

These sampling mechanisms monitor events set given by an instruction type.
They can monitor several events sets at the same time but the number of monitored sets is limited by the hardware capabilities (number of available registers).
Unfortunately, the number of existing events sets that relate to the memory hierarchy is large, because of its complexity.
This makes difficult the task of tracing all the relevant memory accesses with just a single analysis.
One way to lessen the impact of this limitation is to run several times the instrumentation and use advanced methods such as folding~\cite{Servat15Towards} to generate a more accurate summary trace.
Nevertheless, this makes the instrumentation cost grow accordingly.
Moreover, writing (and sometimes) using tools that relies on hardware mechanisms requires a deep knowledge of the processor.
As processors evolve, such tools are hard to maintain and can quickly become outdated.
We regard all these limitations as too constraining for a general purpose memory analysis tool.

Finally other studies rely on hardware modifications either actually implemented or simulated~\cite{Bao08HMTT,Martonosi92MemSpy}.
Although they are eventually able to collect more \emph{precise} traces efficiently, these techniques are limited to hardware developers.
Indeed, to use these hardware extensions one has either to obtain (or build) a prototype or to use a suitable simulator.
Such configuration is not realistic for general purpose memory analysis.

To conclude, existing memory trace collection tools are not able to collect traces \emph{precise} and \emph{detailed} enough to present memory patterns to the end user.
Nevertheless, two tools: \gls{MemProf}~\cite{Lachaize12MemProf} and \gls{Mitos}~\cite{Gimenez14Dissecting} collects \emph{incomplete} and \emph{detailed} traces.
While this is not enough for the kind of analysis we want to run, these tools provides an interesting comparison point.
Both of them collects trace using event sampling (\gls{PEBS} or \gls{IBS}, respectively).
Therefore their trace are \emph{precise} on the parts of the memory that are accessed the most.

\subsection{Memory traces analysis}

Some memory oriented analysis tools such as \gls{Memphis}~\cite{McCurdy10Memphis} and \gls{MemSpy}~\cite{Martonosi92MemSpy} only provide a textual output.
\gls{MemProf}~\cite{Lachaize12MemProf} also provide a command line interface to inspect the trace.
Although these tools highlight relevant informations, it is hard to get an overview of the memory behavior from such output.
The developer might be presented with a huge amount of information and, thus, unable to differentiate normal behaviors from problematic ones nor identify memory patterns.

Several studies~\cite{DeRose01Hardware,DeRose02SIGMA,Bosch00Rivet} rely on generic performance traces and present them in a \emph{data centric} way, in the sense that they correlate the metric values with source code and data structures.
These studies are based on performance counters and present derived metrics such as cache misses or memory bandwidth.
Weyers et al.~\cite{Weyers14Visualization} have a slightly different approach: they present the same kind of data but correlate them with the \gls{NUMA} architecture instead of the data structures.
All these studies can help localizing the issue in the code and find the data structures involved in it.
Furthermore, they provide comprehensive visualization that are easier to understand than plain text traces.
However, they are not able to present the memory patterns, and the developer still has to figure out by itself the nature of the issue.

A part of this limit is due to the fact that the previously cited tools work on generic traces instead of memory traces, hence they do not have the information required to identify access patterns.
Liu et al~\cite{Liu13Datacentric,Liu14Tool} proposed an \gls{HPCToolkit} extension that let the user visualize the number of accesses done by each thread to a data structure.
This visualization gives already more information about data sharing, but the granularity is quite high.
Consequently, we cannot identify patterns inside the data structure nor pattern change over time.
Tao et al~\cite{Tao01Visualizing} proposed a more fine grain  visualization, showing for each page the number of remote and local accesses.
Yet, compared to the previous study they loose the notion of sharing.

Finally, \gls{MemAxes}~\cite{Gimenez14Dissecting}, which is the visualization tool for \gls{Mitos}, provide a unified view of the trace.
This view, presented in \fig{memaxes}, correlates the information collected in the samples (bottom pane) with the architecture (middle pane) and the source code and data structures (left pane).
It enables to do some selection on any part of the visualization to focus on some code or a NUMA node etc.
Furthermore, Husain et al.~\cite{Husain15Relating} have recently added a layer to \gls{MemAxes} that enable to trace simulations and link the simulation visualization to \gls{MemAxes}.
Still, this visualization does not show sharing pattern or access patterns, it only helps identifying the lines of code and data structures responsible for the bad performance (hotspots)
As a result, the user still has to correlate the samples information to understand the nature of the issue and how to fix it.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{memAxes.png}
    \caption[Screenshot from MemAxes.]{Screenshot from MemAxes on the example data trace provided with the
    tool.}
    \label{fig:memaxes}
\end{figure}

\section{Conclusions}
\label{sec:mem-cncl}

The memory subsystems have became more and more complex over the last few decades.
As a result, the way a program allocates and access its memory has a significant impact on the performance.
Eventually, a developer looking for performance must consider the memory access patterns of its application.
Therefore a tool able to collect a memory trace and to display accesses and sharing patterns would be useful for performance optimization.

Most existing memory trace collection tools consider the memory as a monolithic entity and only provide global information such as the bandwidth.
Some tools provide more detailed memory traces.
Nevertheless, they rely either on  hardware based sampling in which case the resulting trace only shows a small subset of the memory, or on hardware modifications and are thus not usable by real life developers.

When it comes to visualizing these traces, many techniques were developed to identify precisely in the code and data structures where performance are suboptimal.
Yet, most of the existing tools are not able to show memory patterns of any kind.
A few advanced tools picture the number of accesses per data structure and per thread or the number of remote accesses per page.
Still this is not sufficient to understand precisely sharing patterns or memory access patterns.

To conclude we need both to collect precise memory traces and to present them to the user in a comprehensive way that enable identification of sharing and access patterns.
% vim: et si sta lbr  sw=4 ts=4 spelllang=en_us
